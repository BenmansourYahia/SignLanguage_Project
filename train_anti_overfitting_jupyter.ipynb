{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ASL Model Training - Anti-Overfitting Version\n",
                "\n",
                "This notebook trains an ASL recognition model with strong regularization to prevent overfitting.\n",
                "\n",
                "**Improvements:**\n",
                "- Early stopping\n",
                "- L2 regularization\n",
                "- Higher dropout rates\n",
                "- Better data augmentation\n",
                "- Larger image size (64x64)\n",
                "- Validation monitoring"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, regularizers\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "TRAINING_DATA_DIR = \"combined_dataset\"\n",
                "IMAGE_SIZE = 64  # Increased from 32 for better detail\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 50  # Max epochs (early stopping will prevent overfitting)\n",
                "LEARNING_RATE = 0.001\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ASL Model Training - ANTI-OVERFITTING VERSION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Max Epochs: {EPOCHS}\")\n",
                "print(f\"Learning Rate: {LEARNING_RATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Check Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify dataset exists\n",
                "if not os.path.exists(TRAINING_DATA_DIR):\n",
                "    print(f\"‚ùå ERROR: '{TRAINING_DATA_DIR}' folder not found!\")\n",
                "    raise FileNotFoundError(f\"Dataset directory not found: {TRAINING_DATA_DIR}\")\n",
                "\n",
                "# Count classes and images\n",
                "classes = [d for d in os.listdir(TRAINING_DATA_DIR) \n",
                "           if os.path.isdir(os.path.join(TRAINING_DATA_DIR, d))]\n",
                "print(f\"‚úì Found {len(classes)} classes: {sorted(classes)}\")\n",
                "\n",
                "# Count total images\n",
                "total_images = sum([len(os.listdir(os.path.join(TRAINING_DATA_DIR, c))) \n",
                "                    for c in classes])\n",
                "print(f\"‚úì Total images: {total_images:,}\")\n",
                "print(f\"‚úì Average per class: {total_images // len(classes):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Data Augmentation Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[1/5] Setting up data augmentation...\")\n",
                "\n",
                "# STRONG augmentation for training (prevents overfitting)\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    rotation_range=20,  # Rotate ¬±20 degrees\n",
                "    width_shift_range=0.15,  # Horizontal shift\n",
                "    height_shift_range=0.15,  # Vertical shift\n",
                "    shear_range=0.1,  # Shearing transformation\n",
                "    zoom_range=0.15,  # Zoom in/out\n",
                "    brightness_range=[0.8, 1.2],  # Brightness variation\n",
                "    horizontal_flip=False,  # Don't flip (ASL is not symmetric)\n",
                "    fill_mode='nearest',\n",
                "    validation_split=0.2  # 80% train, 20% validation\n",
                ")\n",
                "\n",
                "# Validation data - ONLY rescaling (no augmentation)\n",
                "val_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    validation_split=0.2\n",
                ")\n",
                "\n",
                "print(\"‚úì Data augmentation configured\")\n",
                "print(\"  - Rotation: ¬±20¬∞\")\n",
                "print(\"  - Shift: ¬±15%\")\n",
                "print(\"  - Zoom: ¬±15%\")\n",
                "print(\"  - Brightness: 80-120%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Load Training and Validation Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[2/5] Loading training and validation data...\")\n",
                "\n",
                "# Training data generator\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    TRAINING_DATA_DIR,\n",
                "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='sparse',\n",
                "    subset='training',\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "# Validation data generator\n",
                "val_generator = val_datagen.flow_from_directory(\n",
                "    TRAINING_DATA_DIR,\n",
                "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='sparse',\n",
                "    subset='validation',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "num_classes = len(train_generator.class_indices)\n",
                "\n",
                "print(f\"‚úì Training samples: {train_generator.samples:,}\")\n",
                "print(f\"‚úì Validation samples: {val_generator.samples:,}\")\n",
                "print(f\"‚úì Number of classes: {num_classes}\")\n",
                "print(f\"‚úì Class names: {list(train_generator.class_indices.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Visualize Sample Augmented Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize augmented images\n",
                "print(\"Visualizing augmented training samples...\")\n",
                "\n",
                "plt.figure(figsize=(15, 5))\n",
                "sample_batch, sample_labels = next(train_generator)\n",
                "\n",
                "for i in range(min(8, len(sample_batch))):\n",
                "    plt.subplot(2, 4, i + 1)\n",
                "    plt.imshow(sample_batch[i])\n",
                "    class_name = list(train_generator.class_indices.keys())[int(sample_labels[i])]\n",
                "    plt.title(f\"Class: {class_name}\")\n",
                "    plt.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('augmented_samples.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"‚úì Sample images saved as 'augmented_samples.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Build Model with Strong Regularization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[3/5] Building REGULARIZED model architecture...\")\n",
                "\n",
                "model = keras.Sequential([\n",
                "    # Input layer\n",
                "    keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
                "    \n",
                "    # Block 1 - with L2 regularization\n",
                "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
                "                       kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.BatchNormalization(),\n",
                "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
                "                       kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.MaxPooling2D((2, 2)),\n",
                "    keras.layers.Dropout(0.3),\n",
                "    \n",
                "    # Block 2\n",
                "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
                "                       kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.BatchNormalization(),\n",
                "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n",
                "                       kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.MaxPooling2D((2, 2)),\n",
                "    keras.layers.Dropout(0.4),\n",
                "    \n",
                "    # Block 3 - Deeper layer\n",
                "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n",
                "                       kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.BatchNormalization(),\n",
                "    keras.layers.MaxPooling2D((2, 2)),\n",
                "    keras.layers.Dropout(0.5),\n",
                "    \n",
                "    # Dense layers with strong regularization\n",
                "    keras.layers.Flatten(),\n",
                "    keras.layers.Dense(256, activation='relu',\n",
                "                      kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.BatchNormalization(),\n",
                "    keras.layers.Dropout(0.6),\n",
                "    \n",
                "    keras.layers.Dense(128, activation='relu',\n",
                "                      kernel_regularizer=regularizers.l2(0.001)),\n",
                "    keras.layers.Dropout(0.5),\n",
                "    \n",
                "    keras.layers.Dense(num_classes, activation='softmax')\n",
                "])\n",
                "\n",
                "print(\"‚úì Model architecture created\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Compile Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[4/5] Compiling model...\")\n",
                "\n",
                "model.compile(\n",
                "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "print(\"‚úì Model compiled successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Setup Callbacks (Anti-Overfitting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Critical callbacks to prevent overfitting\n",
                "callbacks = [\n",
                "    # Save BEST model (not last)\n",
                "    keras.callbacks.ModelCheckpoint(\n",
                "        filepath='best_model.h5',\n",
                "        monitor='val_accuracy',\n",
                "        save_best_only=True,\n",
                "        mode='max',\n",
                "        verbose=1\n",
                "    ),\n",
                "    \n",
                "    # EARLY STOPPING - stops when validation stops improving\n",
                "    keras.callbacks.EarlyStopping(\n",
                "        monitor='val_accuracy',\n",
                "        patience=5,  # Stop if no improvement for 5 epochs\n",
                "        restore_best_weights=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    \n",
                "    # Reduce learning rate when plateau\n",
                "    keras.callbacks.ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=3,\n",
                "        verbose=1,\n",
                "        min_lr=0.00001\n",
                "    ),\n",
                "]\n",
                "\n",
                "print(\"‚úì Callbacks configured:\")\n",
                "print(\"  - ModelCheckpoint: Save best model\")\n",
                "print(\"  - EarlyStopping: Patience = 5 epochs\")\n",
                "print(\"  - ReduceLROnPlateau: Patience = 3 epochs\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[5/5] Training model...\")\n",
                "print(f\"Max epochs: {EPOCHS} (early stopping enabled)\\n\")\n",
                "\n",
                "# Train the model\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    validation_data=val_generator,\n",
                "    epochs=EPOCHS,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úì Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 11: Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Accuracy plot\n",
                "ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
                "ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
                "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Accuracy')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Loss plot\n",
                "ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
                "ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
                "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Loss')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úì Training history saved as 'training_history.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 12: Evaluate Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate final metrics\n",
                "final_train_acc = history.history['accuracy'][-1]\n",
                "final_val_acc = history.history['val_accuracy'][-1]\n",
                "final_train_loss = history.history['loss'][-1]\n",
                "final_val_loss = history.history['val_loss'][-1]\n",
                "gap = final_train_acc - final_val_acc\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"FINAL RESULTS:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
                "print(f\"Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
                "print(f\"Training Loss:       {final_train_loss:.4f}\")\n",
                "print(f\"Validation Loss:     {final_val_loss:.4f}\")\n",
                "print(f\"\\nOverfitting Gap:     {gap*100:.2f}%\")\n",
                "\n",
                "# Overfitting assessment\n",
                "if gap < 0.05:\n",
                "    print(\"‚úì EXCELLENT: Low overfitting! Model generalizes well.\")\n",
                "elif gap < 0.10:\n",
                "    print(\"‚ö† ACCEPTABLE: Moderate overfitting. Consider more regularization.\")\n",
                "else:\n",
                "    print(\"‚ùå WARNING: High overfitting! Increase dropout or get more data.\")\n",
                "\n",
                "print(f\"\\n‚úì Best model saved as 'best_model.h5'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 13: Convert to TFLite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Converting model to TFLite...\")\n",
                "\n",
                "# Load best model\n",
                "best_model = keras.models.load_model('best_model.h5')\n",
                "\n",
                "# Convert to TFLite\n",
                "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
                "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "tflite_model = converter.convert()\n",
                "\n",
                "# Save TFLite model\n",
                "with open('model.tflite', 'wb') as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "# Get file sizes\n",
                "h5_size = os.path.getsize('best_model.h5') / (1024 * 1024)\n",
                "tflite_size = os.path.getsize('model.tflite') / (1024 * 1024)\n",
                "\n",
                "print(f\"‚úì TFLite model saved as 'model.tflite'\")\n",
                "print(f\"  H5 size: {h5_size:.2f} MB\")\n",
                "print(f\"  TFLite size: {tflite_size:.2f} MB\")\n",
                "print(f\"  Compression: {(1 - tflite_size/h5_size)*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 14: Generate Labels File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save class labels\n",
                "class_labels = sorted(train_generator.class_indices.keys())\n",
                "\n",
                "with open('labels.txt', 'w') as f:\n",
                "    for label in class_labels:\n",
                "        f.write(f\"{label}\\n\")\n",
                "\n",
                "print(f\"‚úì Labels saved to 'labels.txt'\")\n",
                "print(f\"  Classes: {class_labels}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 15: Summary and Next Steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nFiles generated:\")\n",
                "print(\"  ‚úì best_model.h5 - Best Keras model\")\n",
                "print(\"  ‚úì model.tflite - Mobile-optimized model\")\n",
                "print(\"  ‚úì labels.txt - Class labels\")\n",
                "print(\"  ‚úì training_history.png - Training curves\")\n",
                "print(\"  ‚úì augmented_samples.png - Sample images\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"  1. Copy model.tflite to Flutter assets/\")\n",
                "print(\"  2. Copy labels.txt to Flutter assets/\")\n",
                "print(\"  3. Test the model in your Flutter app\")\n",
                "print(\"  4. Monitor real-world performance\")\n",
                "print(\"\\nüéâ Ready for deployment!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}